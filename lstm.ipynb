{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from torchtext.vocab import Vocab, GloVe\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.data import get_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    # 大小写\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace(\"#\" , \" \")\n",
    "    text = text.replace(\".\" , \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "    text = re.sub('https?://[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub('http?://[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub('www.[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    text = re.sub(\"!+\", \"!\", text)\n",
    "\n",
    "#     text = '!'.join(unique_list(text.split('!')))\n",
    "    text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n",
    "    encoded_string = text.encode(\"ascii\", \"ignore\")\n",
    "    decode_string = encoded_string.decode()\n",
    "    return decode_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_list(l):\n",
    "    ulist = []\n",
    "    [ulist.append(x) for x in l if x not in ulist]\n",
    "    return ulist\n",
    "\n",
    "a='I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== I AM AN LOSER == == I AM AN LOSER ==== '\n",
    "a='='.join(unique_list(a.split('=')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Wikipedia tEh Free Enyclopedia! '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning(\" Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Wikipedia tEh Free Enyclopedia!! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_vectors = GloVe(name='twitter.27B', dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\") ## We'll use tokenizer available from PyTorch\n",
    "embeddings = global_vectors.get_vecs_by_tokens(tokenizer(\"?\"), lower_case_backup=True)\n",
    "s1 =  embeddings.tolist()\n",
    "embeddings1 = global_vectors.get_vecs_by_tokens(tokenizer(\"fuck\"), lower_case_backup=True)\n",
    "s2 = embeddings1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-9.1989)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103342</th>\n",
       "      <td>290a507a5d1a2fe6</td>\n",
       "      <td>Hey sexist bitch, shut the fuck up\\n\\nHow dare...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37497</th>\n",
       "      <td>642701583b0cc86b</td>\n",
       "      <td>wikipedia loves cock \\n\\nwikipedia loves cock ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133250</th>\n",
       "      <td>c8edbec909892107</td>\n",
       "      <td>Gogo blows man’s thick cock then gets on all f...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8865</th>\n",
       "      <td>1787e8af29b2563f</td>\n",
       "      <td>NJGW IS AN UNEMPLOYED MOTHER FUCKING BASTARD L...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132265</th>\n",
       "      <td>c3cea5cbd987b22a</td>\n",
       "      <td>Yo Wuhwuzdat u a bitch wit no life. Yeah u hea...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88801</th>\n",
       "      <td>ed948a2fda2f6272</td>\n",
       "      <td>AND TO THE PARTY WHO FUCKS ME, FUCK U! I'LL BE...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32098</th>\n",
       "      <td>554a87fb67e51344</td>\n",
       "      <td>Fuck All Asyriac Nation \\n\\nQamishli belong to...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49353</th>\n",
       "      <td>83f81e6f784b74a6</td>\n",
       "      <td>it's unfortunate that your mother didn't quif ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76543</th>\n",
       "      <td>ccf05c5d4b791836</td>\n",
       "      <td>ur a faggot, fuck off and get a real job and s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91858</th>\n",
       "      <td>f59198179d4305a5</td>\n",
       "      <td>Do you need a verifiable source for that state...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "103342  290a507a5d1a2fe6  Hey sexist bitch, shut the fuck up\\n\\nHow dare...   \n",
       "37497   642701583b0cc86b  wikipedia loves cock \\n\\nwikipedia loves cock ...   \n",
       "133250  c8edbec909892107  Gogo blows man’s thick cock then gets on all f...   \n",
       "8865    1787e8af29b2563f  NJGW IS AN UNEMPLOYED MOTHER FUCKING BASTARD L...   \n",
       "132265  c3cea5cbd987b22a  Yo Wuhwuzdat u a bitch wit no life. Yeah u hea...   \n",
       "88801   ed948a2fda2f6272  AND TO THE PARTY WHO FUCKS ME, FUCK U! I'LL BE...   \n",
       "32098   554a87fb67e51344  Fuck All Asyriac Nation \\n\\nQamishli belong to...   \n",
       "49353   83f81e6f784b74a6  it's unfortunate that your mother didn't quif ...   \n",
       "76543   ccf05c5d4b791836  ur a faggot, fuck off and get a real job and s...   \n",
       "91858   f59198179d4305a5  Do you need a verifiable source for that state...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "103342      1             1        1       0       1              0  \n",
       "37497       1             1        1       0       0              0  \n",
       "133250      1             1        1       0       1              0  \n",
       "8865        1             1        1       0       1              0  \n",
       "132265      1             1        1       0       1              0  \n",
       "88801       1             1        1       1       1              0  \n",
       "32098       1             1        1       1       1              1  \n",
       "49353       1             1        1       0       1              0  \n",
       "76543       1             1        1       0       1              0  \n",
       "91858       1             1        1       0       1              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder_path = 'data/'\n",
    "train = pd.read_csv(data_folder_path + 'train.csv')\n",
    "test_data = pd.read_csv(data_folder_path + 'test.csv')\n",
    "test_labels = pd.read_csv(data_folder_path + 'test_labels.csv')\n",
    "test_data = pd.concat([test_data, test_labels], axis=1)\n",
    "\n",
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train[train[classes[1]] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#seed = 42\n",
    "#train, test_data = train_test_split(train, test_size=0.2, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic rate:  9.584 %\n",
      "severe_toxic rate:  1.0 %\n",
      "obscene rate:  5.295 %\n",
      "threat rate:  0.3 %\n",
      "insult rate:  4.936 %\n",
      "identity_hate rate:  0.88 %\n"
     ]
    }
   ],
   "source": [
    "for cls in classes:\n",
    "    rate = train[cls].sum() / len(train)\n",
    "    rate = np.round(rate*100, 3)\n",
    "    print(cls +' rate: ', rate, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic rate:  -54.253 %\n",
      "severe_toxic rate:  -57.989 %\n",
      "obscene rate:  -55.819 %\n",
      "threat rate:  -58.091 %\n",
      "insult rate:  -55.992 %\n",
      "identity_hate rate:  -57.764 %\n"
     ]
    }
   ],
   "source": [
    "for cls in classes:\n",
    "    rate = test_data[cls].sum() / len(test_data)\n",
    "    rate = np.round(rate*100, 3)\n",
    "    print(cls +' rate: ', rate, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_25168\\256433076.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"comment_text\"].iloc[idx] = global_vectors.get_vecs_by_tokens(token, lower_case_backup=True)\n"
     ]
    }
   ],
   "source": [
    "#l_train = []\n",
    "l_train_drop =[]\n",
    "\n",
    "for idx in range(0, len(train)):\n",
    "    comment = train[\"comment_text\"].iloc[idx]\n",
    "    clean = data_cleaning(comment)\n",
    "    token = tokenizer(clean)\n",
    "    if len(token) == 0:\n",
    "        l_train_drop.append(idx)\n",
    "        continue\n",
    "    train[\"comment_text\"].iloc[idx] = global_vectors.get_vecs_by_tokens(token, lower_case_backup=True)\n",
    "\n",
    "\n",
    "#    row['embeddings'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train.drop(l_train_drop, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 200])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159566"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new['comment_text'][75496].shape\n",
    "train_pad = train_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "134973\n",
      "d1e56e18dadf4a24\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "max_index = 0\n",
    "max_id = 0\n",
    "save = 0\n",
    "for i in range(len(train_new['comment_text'])):\n",
    "    if train_new['id'][i] == 'd1e3a54c4d490fe4':\n",
    "        save = train_new['comment_text'][i]\n",
    "    if len(train_new['comment_text'][i]) > max_len:\n",
    "        max_len = len(train_new['comment_text'][i])\n",
    "        max_index = i\n",
    "        max_id = train_new['id'][i]\n",
    "print(max_len)\n",
    "print(max_index)\n",
    "print(max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_25168\\3349311360.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_pad['comment_text'][i] = temp\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_pad['comment_text'])):\n",
    "    count = 0\n",
    "    for j in range(len(train_pad['comment_text'][i])):\n",
    "        if torch.sum(train_pad['comment_text'][i][j]) == 0:\n",
    "            count += 1\n",
    "    #temp = torch.zeros([max_len+count-len(train_pad['comment_text'][i]),200])\n",
    "    temp = torch.zeros([max_len,200])\n",
    "    count_1 = 0\n",
    "    for j in range(len(train_pad['comment_text'][i])):\n",
    "        if torch.sum(train_pad['comment_text'][i][j]) != 0:\n",
    "            #temp = torch.cat((temp, train_pad['comment_text'][i][j].reshape(1,-1)))\n",
    "            temp[max_len+count-len(train_pad['comment_text'][i]) + count_1] = train_pad['comment_text'][i][j]\n",
    "            count_1 += 1\n",
    "    train_pad['comment_text'][i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43msave\u001b[49m)):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(save[i]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      4\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save' is not defined"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(save)):\n",
    "    if torch.sum(save[i]) == 0:\n",
    "        count += 1\n",
    "        \n",
    "print(len(save))\n",
    "temp = torch.zeros([max_len,200])\n",
    "count_10 = 0\n",
    "for i in range(len(save)):\n",
    "    if torch.sum(save[i]) != 0:\n",
    "        #save_non0 = torch.cat((save_non0, save[i].reshape(1,-1)))\n",
    "        temp[max_len+count-len(save) + count_10] = save[i]\n",
    "        count_10 += 1\n",
    "save_non0 = temp\n",
    "print(save_non0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_non0[282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_non0 = save_non0[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 200])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_non0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new.to_csv('train_after_tokenizer_clean_indexed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_after_tokenizer_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1937, -0.1251, -0.5924,  ...,  0.7811, -0.1109, -0.2980],\n",
      "        [ 0.2539,  0.3569,  0.2210,  ...,  0.1067, -0.1587,  0.1287],\n",
      "        [ 0.4935,  0.3570,  0.6607,  ...,  0.1771, -0.5369, -0.2970],\n",
      "        ...,\n",
      "        [ 0.1430, -0.4178,  0.2529,  ..., -0.7805,  0.3792,  0.4934],\n",
      "        [ 0.1588,  0.2819, -0.4703,  ...,  0.8744, -0.5519, -0.1927],\n",
      "        [-0.0218, -0.0115, -0.4395,  ...,  0.4765,  0.2968,  0.6029]])\n",
      "PackedSequence(data=tensor([-0.1937,  0.2539,  0.4935,  ...,  0.4934, -0.1927,  0.6029]), batch_sizes=tensor([51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[ 0.2442,  0.3383,  0.5033,  ..., -0.4469, -0.2331,  0.0522],\n",
      "        [-0.5164,  0.0647, -0.3937,  ..., -0.2190, -0.1617, -0.3781],\n",
      "        [ 0.0677, -0.2674,  0.0728,  ..., -0.4911, -0.2488,  0.6093],\n",
      "        ...,\n",
      "        [ 0.1676, -0.2118,  0.0583,  ..., -0.2236, -0.5680, -0.0579],\n",
      "        [ 0.4132,  0.1357, -0.3712,  ...,  0.5137,  0.7586,  0.2672],\n",
      "        [ 0.3119, -0.0463, -0.1282,  ..., -0.7497, -0.5116,  0.2095]])\n",
      "PackedSequence(data=tensor([ 0.2442, -0.5164,  0.0677,  ..., -0.0579,  0.2672,  0.2095]), batch_sizes=tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[ 0.5244,  0.2295, -0.2096,  ..., -0.3582,  0.2474, -0.4814],\n",
      "        [-0.1859,  0.1115, -0.0507,  ...,  0.2287, -0.2876,  0.1388],\n",
      "        [ 0.3927, -0.0842, -0.6075,  ...,  0.0939, -0.5851,  0.1954],\n",
      "        ...,\n",
      "        [ 0.4935,  0.3570,  0.6607,  ...,  0.1771, -0.5369, -0.2970],\n",
      "        [-0.0516, -0.0598, -0.1052,  ...,  0.5716, -0.5702,  0.1067],\n",
      "        [-0.4089,  0.3150, -0.8155,  ...,  0.0263, -0.3000,  0.5652]])\n",
      "PackedSequence(data=tensor([ 0.5244, -0.1859,  0.3927,  ..., -0.2970,  0.1067,  0.5652]), batch_sizes=tensor([47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[ 0.2453,  0.0183, -0.3511,  ..., -0.2389, -0.3607,  0.1470],\n",
      "        [ 0.0564,  0.4954,  0.1844,  ...,  0.6360, -0.1888, -0.0356],\n",
      "        [ 0.0494,  0.6521,  0.0852,  ..., -0.2740,  0.3942, -0.1203],\n",
      "        ...,\n",
      "        [-0.0540,  0.1507, -0.7549,  ...,  0.0609, -0.7760, -0.6108],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4382,  0.3922,  0.0022,  ...,  0.4480, -0.2087,  0.0090]])\n",
      "PackedSequence(data=tensor([ 0.2453,  0.0564,  0.0494,  ..., -0.6108,  0.0000,  0.0090]), batch_sizes=tensor([119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,\n",
      "        119, 119, 119, 119]), sorted_indices=None, unsorted_indices=None)\n",
      "tensor([[ 0.1964,  0.6715,  0.0063,  ..., -0.3081, -0.1231,  0.0489],\n",
      "        [ 0.3927, -0.0842, -0.6075,  ...,  0.0939, -0.5851,  0.1954],\n",
      "        [ 0.3607, -0.1164, -0.3302,  ..., -0.1843,  0.5323,  0.3184],\n",
      "        ...,\n",
      "        [ 0.5516, -0.1907,  0.2794,  ..., -0.3413,  0.3175,  0.2361],\n",
      "        [-0.7518, -0.2664,  0.4026,  ..., -0.1010, -0.0022, -0.1353],\n",
      "        [ 0.3621,  0.0846, -0.2436,  ..., -0.6513, -0.7392,  0.5278]])\n",
      "PackedSequence(data=tensor([ 0.1964,  0.3927,  0.3607,  ...,  0.2361, -0.1353,  0.5278]), batch_sizes=tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
    "\n",
    "for idx in range(0, 5):\n",
    "    comment = train[\"comment_text\"][idx]\n",
    "    print(comment)\n",
    "    pad = pack_sequence(comment)\n",
    "    print(pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pad.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 200])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:66] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 631626054400 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m padded \u001b[38;5;241m=\u001b[39m \u001b[43mpack_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomment_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/COMP9444/lib/python3.9/site-packages/torch/nn/utils/rnn.py:482\u001b[0m, in \u001b[0;36mpack_sequence\u001b[0;34m(sequences, enforce_sorted)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Packs a list of variable length Tensors\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03mConsecutive call of the next functions: ``pad_sequence``, ``pack_padded_sequence``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m    a :class:`PackedSequence` object\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor([v\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m sequences])\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_padded_sequence(\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m, lengths, enforce_sorted\u001b[38;5;241m=\u001b[39menforce_sorted)\n",
      "File \u001b[0;32m~/miniconda3/envs/COMP9444/lib/python3.9/site-packages/torch/nn/utils/rnn.py:396\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    392\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:66] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 631626054400 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "padded = pack_sequence(train[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
